base:
  dataset: 'moorfields' # in ['kaggle_1024', 'kaggle_224', 'kaggle_512']
  data_index: null # alternative way to build dataset. check README for more details
  device: cuda 
  random_seed: 0 #819, 3407
  test: False #True, False
  sample: 0
  
save_paths:

data_paths:

data:
  num_classes: 2
  threshold: 2 
  binary: True
  binary_type: onset1
  input_size: 512
  # mean and std for the moorfields 512 dataset (good quality images)
  mean: [0.4604, 0.2410, 0.1143]
  std: [0.2983, 0.1695, 0.1147]
  #mean: [0.41326871514320374, 0.2723627984523773, 0.18590997159481049] # 512
  #std: [0.29345420002937317, 0.20033970475196838, 0.15474912524223328] # 512
  #mean: [0.4126319885253906, 0.2717236578464508, 0.18526685237884521] # 224
  #std: [0.29319170117378235, 0.20009475946426392, 0.1545303463935852] # 224
  sampling_strategy: instance_balanced # instance_balanced / class_balanced / progressively_balanced. ref: https://arxiv.org/abs/1910.09217
  sampling_weights_decay_rate: 0.9 # if sampling_strategy is progressively_balanced, sampling weight will change from class_balanced to instance_balanced
  augmentation: 'baseline' #['baseline', 'other']
  data_augmentation: # available operations are list in 'data_augmentation_args' below
    - random_crop
    - horizontal_flip
    - vertical_flip
    - color_distortion
    - rotation
    - translation

train:
  network: resnet50 # [bagnet33, resnet50 (=> v1)] available networks are list in networks.yaml 
  version: v2  # v1 = bagnet without reg, v2 = with reg
  sparsity: False  # [True, False]
  batch_reg_record: False   # just to monitor the regularization after every epoch
  pretrained: true # load weights from pre-trained model training on ImageNet
  checkpoint: null # load weights from other pretrained model
  epochs: 50
  batch_size: 30
  num_workers: 8 # 0, 2, 4 good = 2 number of cpus used to load data at each step
  criterion: cross_entropy # [cross_entropy, mean_square_error ] available criterions are list in 'criterion_args' below
  loss_weight: null # null / balance / dynamic / list with shape num_classes. Weights for loss function. Don't use it with weighted sampling!
  loss_weight_decay_rate: 0 # if loss_weights is dynamic, loss weight will decay from balance to equivalent weights
  warmup_epochs: 0 # set to 0 to disable warmup
  kappa_prior: false # save model with higher kappa or higher accuracy in validation set
  save_interval: 5 # the epoch interval of saving model
  eval_interval: 1 # the epoch interval of evaluating model on val dataset
  sample_view: false # save and visualize a batch of images on Tensorboard
  sample_view_interval: 100 # the steps interval of saving samples on Tensorboard. Note that frequently saving images will slow down the training speed.
  pin_memory: true # enables fast data transfer to CUDA-enabled GPUs
  #lambda_l1: 0.00002 # [v2_bad 0.00008, 0.00012, 0.00018, 0.0006, 0.00002] [v1 fair 0.0001, 0.00008]
  #lambda_l1: 0.000007 # multi [v2 0.00002, 0.000013, 0.000006, 0.000004] [v1 fair 0.00008, 0.00007, 0.00005] good=0.000007
  lambda_l1: 0.000004 # [0.00001, 0.00002, 0.000009] Onset2
  
solver:
  optimizer: SGD # SGD / ADAM
  learning_rate: 0.001 # initial learning rate
  lr_scheduler: clipped_cosine # [cosine, clipped_cosine] available schedulers are list in 'scheduler_args' below. please remember to update scheduler_args when number of epoch changes.
  momentum: 0.9 # only for SGD. set to 0 to disable momentum
  nesterov: true # only for SGD.
  weight_decay: 0.0005 # set to 0 to disable weight decay

criterion_args:
  cross_entropy: {}
  mean_square_error: {}
  mean_absolute_error: {}
  smooth_L1: {}
  kappa_loss:
    num_classes: 5 # [2, 5]
  focal_loss:
    alpha: 5
    reduction: mean

# please refer to documents of torch.optim
scheduler_args:
  exponential:
    gamma: 0.6 # multiplicative factor of learning rate decay
  multiple_steps:
    milestones: [15, 25, 45]
    gamma: 0.1 # multiplicative factor of learning rate decay
  cosine:
    T_max: 50 # maximum number of iterations
    eta_min: 0 # minimum learning rate
  reduce_on_plateau:
    mode: min
    factor: 0.1 # new learning rate = factor * learning rate
    patience: 5 # number of epochs with no improvement after which learning rate will be reduced.
    threshold: 0.0001 # threshold for measuring the new optimum
    eps: 0.00001 # minimal decay applied to learning rate
  clipped_cosine:
    T_max: 50
    min_lr: 0.0001 #

data_augmentation_args:
  horizontal_flip:
    prob: 0.5
  vertical_flip:
    prob: 0.5
  color_distortion:
    prob: 0.5
    brightness: 0.2
    contrast: 0.2
    saturation: 0.2
    hue: 0.1
  random_crop: # randomly crop and resize to input_size
    prob: 0.5
    scale: [0.87, 1.15] # range of size of the origin size cropped
    ratio: [0.7, 1.3] # range of aspect ratio of the origin aspect ratio cropped
  rotation:
    prob: 0.5
    degrees: [-180, 180]
  translation:
    prob: 0.5
    range: [0.2, 0.2]
  grayscale: # randomly convert image to grayscale
    prob: 0.5
  gaussian_blur: # only available for torch version >= 1.7.1.
    prob: 0.2
    kernel_size: 7
    sigma: 0.5
  value_fill: 0 # NOT a data augmentation operation. pixel fill value for the area outside the image